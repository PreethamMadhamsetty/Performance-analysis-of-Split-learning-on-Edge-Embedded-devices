{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18f63ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = 1 # number of clients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90097ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/keras/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import struct\n",
    "import socket\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e08c4324",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/Users/tej/models/cifar10_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a48287a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "torch.manual_seed(777)\n",
    "if device ==\"cuda:0\":\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dd1246d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_order(start from 0): 0\n"
     ]
    }
   ],
   "source": [
    "client_order = int(input(\"client_order(start from 0): \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5f473771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Number of clients: 1\n",
      "Number of images for Client 1: 16666\n",
      "Class 0 Test Accuracy: 0.599\n",
      "Class 1 Test Accuracy: 0.669\n",
      "Class 2 Test Accuracy: 0.468\n",
      "Class 3 Test Accuracy: 0.401\n",
      "Class 4 Test Accuracy: 0.461\n",
      "Class 5 Test Accuracy: 0.364\n",
      "Class 6 Test Accuracy: 0.637\n",
      "Class 7 Test Accuracy: 0.516\n",
      "Class 8 Test Accuracy: 0.616\n",
      "Class 9 Test Accuracy: 0.633\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define the model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):  # Corrected here\n",
    "        super(SimpleCNN, self).__init__()  # Corrected here\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc = nn.Linear(32 * 16 * 16, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = x.view(-1, 32 * 16 * 16)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Divide the dataset into three equal parts for three clients\n",
    "total_images = len(trainset)\n",
    "split_points = [total_images // 3, 2 * total_images // 3]\n",
    "\n",
    "client1_dataset = torch.utils.data.Subset(trainset, range(split_points[0]))\n",
    "#client2_dataset = torch.utils.data.Subset(trainset, range(split_points[0], split_points[1]))\n",
    "#client3_dataset = torch.utils.data.Subset(trainset, range(split_points[1], total_images))\n",
    "\n",
    "client1_loader = torch.utils.data.DataLoader(client1_dataset, batch_size=32, shuffle=True)\n",
    "#client2_loader = torch.utils.data.DataLoader(client2_dataset, batch_size=32, shuffle=True)\n",
    "#client3_loader = torch.utils.data.DataLoader(client3_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Print the number of clients and images for each client\n",
    "print(f\"Number of clients: 1\")\n",
    "print(f\"Number of images for Client 1: {len(client1_dataset)}\")\n",
    "#print(f\"Number of images for Client 2: {len(client2_dataset)}\")\n",
    "#print(f\"Number of images for Client 3: {len(client3_dataset)}\")\n",
    "\n",
    "# Initialize the global model\n",
    "global_model = SimpleCNN()\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(global_model.parameters(), lr=0.01)\n",
    "\n",
    "# Training on Client 1\n",
    "for epoch in range(epochs):\n",
    "    for data, target in client1_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = global_model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Training on Client 2\n",
    "#for epoch in range(epochs):\n",
    "#    for data, target in client2_loader:\n",
    "#        optimizer.zero_grad()\n",
    "#        output = global_model(data)\n",
    "#        loss = criterion(output, target)\n",
    "#        loss.backward()\n",
    "#        optimizer.step()\n",
    "\n",
    "# Training on Client 3\n",
    "#for epoch in range(epochs):\n",
    "#   for data, target in client3_loader:\n",
    "#        optimizer.zero_grad()\n",
    "#        output = global_model(data)\n",
    "#        loss = criterion(output, target)\n",
    "#        loss.backward()\n",
    "#        optimizer.step()\n",
    "\n",
    "# Evaluate the global model\n",
    "class_correct = [0] * 10\n",
    "class_total = [0] * 10\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False):\n",
    "        output = global_model(data)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        for i in range(10):\n",
    "            class_total[i] += (target == i).sum().item()\n",
    "            class_correct[i] += (predicted == target)[target == i].sum().item()\n",
    "\n",
    "for i in range(10):\n",
    "    accuracy = class_correct[i] / class_total[i]\n",
    "    print(f\"Class {i} Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e569392a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Total number of images in the dataset\n",
    "total_images = 16666\n",
    "\n",
    "# Number of clients\n",
    "num_clients = 1\n",
    "\n",
    "# Number of images per client\n",
    "num_traindata = total_images // num_clients\n",
    "\n",
    "# Client order (0 for the first client, 1 for the second, etc.)\n",
    "client_order = 0  # Replace with the current client index\n",
    "\n",
    "indices = list(range(total_images))\n",
    "\n",
    "# Calculate the part of the dataset for the current client\n",
    "part_tr = indices[num_traindata * client_order : num_traindata * (client_order + 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36b1e423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10 (root=root_path, train=True, download=True, transform=transform)\n",
    "\n",
    "trainset_sub = Subset(trainset, part_tr)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset_sub, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10 (root=root_path, train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57be3bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 32, 32])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = next(iter(train_loader))\n",
    "print(x_train.size())\n",
    "print(y_train.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4997949e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4167\n"
     ]
    }
   ],
   "source": [
    "total_batch = len(train_loader)\n",
    "print(total_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5248f16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Nov  1 14:23:31 2018\n",
    "@author: tshzzz\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def conv_dw_client(inplane,stride=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inplane,inplane,kernel_size = 3,groups = inplane,stride=stride,padding=1),\n",
    "        nn.BatchNorm2d(inplane),\n",
    "        nn.ReLU()  \n",
    "        )\n",
    "\n",
    "def conv_bw(inplane,outplane,kernel_size = 3,stride=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inplane,outplane,kernel_size = kernel_size,groups = 1,stride=stride,padding=1),\n",
    "        nn.BatchNorm2d(outplane),\n",
    "        nn.ReLU() \n",
    "        )\n",
    "\n",
    "\n",
    "class MobileNet(nn.Module):\n",
    "    \n",
    "    def __init__(self,num_class=10):\n",
    "        super(MobileNet,self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(conv_bw(3,32,3,1))\n",
    "        layers.append(conv_dw_client(32,1))\n",
    "#         layers.append(conv_dw(64,128,2))\n",
    "#         layers.append(conv_dw(128,128,1))\n",
    "#         layers.append(conv_dw(128,256,2))\n",
    "#         layers.append(conv_dw(256,256,1))\n",
    "#         layers.append(conv_dw(256,512,2))\n",
    "\n",
    "#         for i in range(5):\n",
    "#             layers.append(conv_dw(512,512,1))\n",
    "#         layers.append(conv_dw(512,1024,2))\n",
    "#         layers.append(conv_dw(1024,1024,1))\n",
    "\n",
    "#         self.classifer = nn.Sequential(\n",
    "#                 nn.Dropout(0.5),\n",
    "#                 nn.Linear(1024,num_class)\n",
    "#                 )\n",
    "        self.feature = nn.Sequential(*layers)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.feature(x)\n",
    "#         out = out.mean(3).mean(2)\n",
    "#         out = out.view(-1,1024)\n",
    "#         out = self.classifer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c77f729e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNet(\n",
      "  (feature): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mobilenet_client = MobileNet().to(device)\n",
    "print(mobilenet_client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c7b6a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "\n",
    "# summary(mobilenet_client, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02b562f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 20  # default\n",
    "lr = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(mobilenet_client.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe639f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_msg(sock, msg):\n",
    "    # prefix each message with a 4-byte length in network byte order\n",
    "    msg = pickle.dumps(msg)\n",
    "    msg = struct.pack('>I', len(msg)) + msg\n",
    "    sock.sendall(msg)\n",
    "\n",
    "def recv_msg(sock):\n",
    "    # read message length and unpack it into an integer\n",
    "    raw_msglen = recvall(sock, 4)\n",
    "    if not raw_msglen:\n",
    "        return None\n",
    "    msglen = struct.unpack('>I', raw_msglen)[0]\n",
    "    # read the message data\n",
    "    msg =  recvall(sock, msglen)\n",
    "    msg = pickle.loads(msg)\n",
    "    return msg\n",
    "\n",
    "def recvall(sock, n):\n",
    "    # helper function to receive n bytes or return None if EOF is hit\n",
    "    data = b''\n",
    "    while len(data) < n:\n",
    "        packet = sock.recv(n - len(data))\n",
    "        if not packet:\n",
    "            return None\n",
    "        data += packet\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a6b2f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IP address: 10.0.0.94\n"
     ]
    }
   ],
   "source": [
    "host = input(\"IP address: \")\n",
    "port = 10081\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1d5ea01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timmer start!\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()    # store start time\n",
    "print(\"timmer start!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e033cf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = socket.socket()\n",
    "s.connect((host, port))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d215c4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = recv_msg(s)   # get epoch\n",
    "msg = total_batch\n",
    "send_msg(s, msg)   # send total_batch of train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f63901c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████████████████████████| 4167/4167 [22:53<00:00,  3.03it/s]\n"
     ]
    }
   ],
   "source": [
    "for e in range(epoch):\n",
    "    client_weights = recv_msg(s)\n",
    "    mobilenet_client.load_state_dict(client_weights)\n",
    "    mobilenet_client.eval()\n",
    "    for i, data in enumerate(tqdm(train_loader, ncols=100, desc='Epoch '+str(e+1))):\n",
    "        x, label = data\n",
    "        x = x.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = mobilenet_client(x)\n",
    "        client_output = output.clone().detach().requires_grad_(True)\n",
    "        msg = {\n",
    "            'client_output': client_output,\n",
    "            'label': label\n",
    "        }\n",
    "        send_msg(s, msg)\n",
    "        client_grad = recv_msg(s)\n",
    "        output.backward(client_grad)\n",
    "        optimizer.step()\n",
    "    send_msg(s, mobilenet_client.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f00dcc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WorkingTime of  cpu : 1631.8849160671234 sec\n"
     ]
    }
   ],
   "source": [
    " end_time = time.time()  #store end time\n",
    "print(\"WorkingTime of \",device ,\": {} sec\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0777bdf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
